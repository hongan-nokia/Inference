# --------------------------------------------------------
#  main_split_fixed_v5.py
#
#  è¯´æ˜ï¼š
#   1. å‡è®¾ä½ æ‹†åˆ†æˆï¼šå‰ 12 å±‚ â†’ stage1ï¼›å 20 å±‚ â†’ stage2ï¼Œå…± 32 å±‚ Llama æ¨¡å‹ã€‚
#   2. å…ˆåŠ è½½å®Œæ•´çš„ HF LlamaForCausalLMï¼Œç„¶åä»ä¸­â€œå‰¥ç¦»â€å­æ¨¡å—ï¼Œ
#      é¿å…ç›´æ¥æ„é€  LlamaDecoderLayer å¯¼è‡´ rotaryâ€emb è¿”å› None çš„é—®é¢˜ã€‚
#   3. åœ¨è°ƒç”¨æ¯ä¸ª LlamaDecoderLayer æ—¶ä¸ä¼  position_idsï¼Œä¾èµ–å®ƒå†…éƒ¨â€œæ ¹æ® hidden_states ç”Ÿæˆ 1D position_idsâ€çš„é»˜è®¤é€»è¾‘ã€‚
#   4. æ‰‹åŠ¨æ„é€  fcauseal_maskï¼ˆbool ä¸‹ä¸‰è§’ + paddingï¼‰ï¼Œä¼ ç»™ attention_mask å‚æ•°å³å¯ã€‚
#
#  ä½ åªéœ€ä¿®æ”¹ä¸‹é¢ä¸¤è¡Œè·¯å¾„ï¼Œå°±èƒ½ç›´æ¥è¿è¡Œï¼š
#    base_model         : æŒ‡å‘ llama3_8B_Base ç›®å½•ï¼ˆåŒ…å« config.json/pytorch_model.bin/tokenizer ç­‰ï¼‰
#    split_weights_dir  : æŒ‡å‘å­˜æ”¾ model_stage1.safetensorsã€model_stage2.safetensors çš„æ–‡ä»¶å¤¹
# --------------------------------------------------------

import os
import torch
from torch import nn
from safetensors.torch import load_file
from transformers import AutoTokenizer, LlamaConfig, LlamaForCausalLM

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ã€è¯·åœ¨æ­¤å¤„ä¿®æ”¹ä¸ºä½ æœ¬åœ°å®é™…çš„è·¯å¾„ã€‘
base_model         = "/guozhanqiu/LLModels/llama3_8B_Base"
split_weights_dir  = "/guozhanqiu/model_layers"
stage1_path        = os.path.join(split_weights_dir, "model_stage1.safetensors")
stage2_path        = os.path.join(split_weights_dir, "model_stage2.safetensors")
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 1. åŠ è½½å®Œæ•´çš„ LlamaForCausalLMï¼ˆä»…ç”¨äºâ€œå‰¥ç¦»â€å­æ¨¡å—ç»“æ„ï¼‰
#    ç”¨ float16 ä¸”å…ˆæ”¾åˆ° CPUï¼Œä»¥èŠ‚çœæ˜¾å­˜
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
print("1ï¸âƒ£ è½½å…¥å®Œæ•´çš„ LlamaForCausalLMï¼ˆä»…ç”¨äºè·å–å­æ¨¡å—ç»“æ„ï¼‰â€¦â€¦")
full_model = LlamaForCausalLM.from_pretrained(
    base_model,
    torch_dtype=torch.float16,
    device_map="cpu",
    low_cpu_mem_usage=True
)
config = full_model.config
tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=False)
print(f"   â†’ æ¨¡å‹å…±æœ‰ {config.num_hidden_layers} å±‚ Transformerï¼ˆåº”ä¸º 32ï¼‰ã€‚")

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 2. æ˜ç¡®æ‹†åˆ†å±‚æ•°ï¼šå‰ 12 å±‚å±äº Stage1ï¼Œåï¼ˆconfig.num_hidden_layers âˆ’ 12ï¼‰å±‚å±äº Stage2
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
client_layers = 12
assert config.num_hidden_layers == 32, "Expect num_hidden_layers=32"
server_layers = config.num_hidden_layers - client_layers  # 32 - 12 = 20
print(f"2ï¸âƒ£ æ‹†åˆ†ï¼šå‰ {client_layers} å±‚ â†’ Stage1ï¼›å {server_layers} å±‚ â†’ Stage2ã€‚")

# 2.1 ä» full_model æ‹†å‡º embed_tokens
embed_tokens = full_model.model.embed_tokens

# 2.2 ä» full_model æ‹†å‡ºå‰ client_layers ä¸ª decoder å±‚ï¼ˆlayers[0..11]ï¼‰
stage1_layers = [full_model.model.layers[i] for i in range(client_layers)]

# 2.3 ä» full_model æ‹†å‡ºå server_layers ä¸ª decoder å±‚ï¼ˆlayers[12..31]ï¼‰
stage2_layers = [full_model.model.layers[i] for i in range(client_layers, config.num_hidden_layers)]

# 2.4 ä» full_model æ‹†å‡ºæœ€åçš„ RMSNorm
rms_norm = full_model.model.norm

# 2.5 ä» full_model æ‹†å‡º lm_head
lm_head = full_model.lm_head

# é‡Šæ”¾ full_model ä»¥èŠ‚çœå†…å­˜ï¼Œä¿ç•™å­æ¨¡å—å¼•ç”¨
del full_model

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 3. å®šä¹‰ Stage1/Stage2 å­æ¨¡å— Wrapper ç±»
#    - åœ¨ forward æ—¶**ä¸ä¼  position_ids**ï¼Œè®© HF å®˜æ–¹å†…éƒ¨è‡ªåŠ¨å¤„ç† rotary embedding çš„ position_ids
#    - æ‰‹åŠ¨æ„é€  bool å‹çš„ causal_maskï¼ˆä¸‹ä¸‰è§’ + paddingï¼‰ï¼Œå¹¶ä¼ ç»™ attention_mask
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
class LlamaStage1(nn.Module):
    """
    Stage1ï¼šembed_tokens + å‰ client_layers å±‚ LlamaDecoderLayer
    """
    def __init__(self, embed_tokens: nn.Embedding, layers: nn.ModuleList):
        super().__init__()
        self.embed_tokens = embed_tokens
        self.layers = nn.ModuleList(layers)

    def forward(self,
                input_ids: torch.LongTensor,      # [B, L]
                attention_mask: torch.Tensor,     # [B, L]
                past_key_values=None,
                return_dict: bool = False,
                output_attentions: bool = False,
                use_cache: bool = False
               ):
        """
        å¤åˆ¶ Hugging Face LlamaModel.forward çš„æµç¨‹ï¼Œåªè·‘åˆ°ç¬¬ client_layers å±‚ç»“æŸã€‚
        ä¸»åŠ¨**ä¸ä¼  position_ids**ï¼Œè®© LlamaDecoderLayer å†…éƒ¨è‡ªåŠ¨æ„å»º 1D position_idsã€‚
        è¿”å›ï¼šlast_hidden_state, past_key_values (è‹¥ use_cache), hidden_states/attns (è‹¥è¾“å‡ºå®ƒä»¬),
              causal_mask, seq_len (ä¸ºä¸‹æ¸¸ Stage2 ç”¨)
        """
        # 1) embed_tokens
        hidden_states = self.embed_tokens(input_ids)  # [B, L, H]

        # 2) å‡†å¤‡ bool å‹çš„ 4D ä¸‹ä¸‰è§’å› æœæ©ç 
        B, L = input_ids.shape
        #    ä¸‹ä¸‰è§’ [L, L]ï¼Œdtype=torch.bool
        tril = torch.tril(torch.ones((L, L), dtype=torch.bool, device=input_ids.device))  # [L, L]
        #    æ‰©å±•åˆ° [B, 1, L, L]
        causal_mask = tril.unsqueeze(0).unsqueeze(1).expand(B, 1, L, L)                  # [B, 1, L, L]
        #    padding_maskï¼šattention_mask.bool() â†’ [B, L] â†’ reshape æˆ [B, 1, 1, L]
        padding_mask = attention_mask.bool().view(B, 1, 1, L)                            # [B, 1, 1, L]
        #    ç”¨â€œé€»è¾‘ä¸â€å±è”½ padding
        causal_mask = causal_mask & padding_mask                                         # [B, 1, L, L]

        all_hidden_states = () if output_attentions or return_dict else None
        all_self_attns   = () if output_attentions else None
        next_cache       = None

        # 3) é€å±‚è¿‡å‰ client_layers å±‚ï¼›**ä¸ä¼  position_ids, cache_position**
        for layer in self.layers:
            if output_attentions:
                all_hidden_states += (hidden_states,)

            layer_outputs = layer(
                hidden_states=hidden_states,
                attention_mask=causal_mask,   # ä¼ å…¥ bool å‹çš„ causal_mask
                output_attentions=output_attentions,
                use_cache=use_cache
            )
            # layer_outputs[0] å§‹ç»ˆæ˜¯æ›´æ–°åçš„ hidden_states
            hidden_states = layer_outputs[0]  # [B, L, H]
            if use_cache:
                # è‹¥ä¸è¾“å‡º attnï¼Œé‚£ layer_outputs[1] å°±æ˜¯ present_key_value
                next_cache = layer_outputs[1] if not output_attentions else layer_outputs[2]
            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        # è¿”å›ç»“æœï¼šæˆ‘ä»¬é¢å¤–éœ€è¦æŠŠ causal_mask å’Œå½“å‰åºåˆ—é•¿åº¦ L ä¼ ä¸‹å»
        if return_dict:
            return {
                "last_hidden_state": hidden_states,
                "past_key_values": next_cache,
                "hidden_states": all_hidden_states,
                "attentions": all_self_attns,
                "causal_mask": causal_mask,
                "seq_len": L
            }
        else:
            out = (hidden_states, next_cache, all_hidden_states, all_self_attns, causal_mask, L)
            return out[:3] + (causal_mask, L)


class LlamaStage2(nn.Module):
    """
    Stage2ï¼šåç»­ server_layers ä¸ª LlamaDecoderLayer + RMSNorm
    """
    def __init__(self, layers: nn.ModuleList, rms_norm: nn.Module):
        super().__init__()
        self.layers = nn.ModuleList(layers)
        self.norm   = rms_norm

    def forward(self,
                hidden_states: torch.FloatTensor,  # [B, L, H] æ¥è‡ª Stage1
                causal_mask: torch.Tensor,         # [B, 1, L, L]ï¼Œdtype=torch.bool
                seq_len: int,                      # Stage1 å½“å‰çš„åºåˆ—é•¿åº¦
                past_key_values=None,
                output_attentions: bool = False,
                use_cache: bool = False
               ):
        all_hidden_states = () if output_attentions or use_cache else None
        all_self_attns   = () if output_attentions else None
        next_cache       = None

        # 1) é€å±‚è¿‡å server_layers å±‚ï¼›**ä¸ä¼  position_ids, cache_position**
        for layer in self.layers:
            if output_attentions:
                all_hidden_states += (hidden_states,)

            layer_outputs = layer(
                hidden_states=hidden_states,
                attention_mask=causal_mask,   # å¸ƒå°”å‹ causal_mask
                output_attentions=output_attentions,
                use_cache=use_cache
            )
            hidden_states = layer_outputs[0]
            if use_cache:
                next_cache = layer_outputs[1] if not output_attentions else layer_outputs[2]
            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        # 2) æœ€åä¸€å±‚åš RMSNorm
        hidden_states = self.norm(hidden_states)  # [B, L, H]

        if use_cache or output_attentions:
            if output_attentions:
                return hidden_states, next_cache, all_hidden_states, all_self_attns
            return hidden_states, next_cache
        return hidden_states

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 4. æ„å»º Stage1/Stage2 å­æ¨¡å—ï¼Œå¹¶åŠ è½½æ‹†åˆ†åçš„ safetensors æƒé‡
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
print("3ï¸âƒ£ æ„å»º Stage1/Stage2 å­æ¨¡å—ï¼Œå¹¶åŠ è½½æ‹†åˆ†æƒé‡â€¦â€¦")
# 4.1 å®ä¾‹åŒ–å­æ¨¡å—ï¼ˆembed_tokens, layers å·²ç»ä» full_model æ‹†å‡ºï¼‰
stage1_net = LlamaStage1(embed_tokens=embed_tokens, layers=stage1_layers)
stage2_net = LlamaStage2(layers=stage2_layers, rms_norm=rms_norm)

# 4.2 lm_head ä¹Ÿå·²ä» full_model æ‹†å‡ºï¼Œåé¢ä¸€å¹¶è½¬åˆ° GPU

# 4.3 åŠ è½½ model_stage1.safetensors
print("   â†’ è½½å…¥ model_stage1.safetensors â€¦â€¦")
state_stage1 = load_file(stage1_path)
client_state_dict = {}
for full_key, tensor in state_stage1.items():
    # safetensors çš„ key é€šå¸¸å¸¦å‰ç¼€ "model."
    new_key = full_key.removeprefix("model.")
    client_state_dict[new_key] = tensor
# æŠŠå‰ 12 å±‚ + embed_tokens çš„æƒé‡ load è¿› stage1_net
stage1_net.load_state_dict(client_state_dict, strict=False)

# 4.4 åŠ è½½ model_stage2.safetensors
print("   â†’ è½½å…¥ model_stage2.safetensors â€¦â€¦")
state_stage2 = load_file(stage2_path)
server_state_dict = {}
for full_key, tensor in state_stage2.items():
    new_key = full_key.removeprefix("model.")
    if new_key == "lm_head.weight":
        # lm_head.weight å•ç‹¬èµ‹ç»™ lm_headï¼ˆFloat32 å†è½¬æ¢åˆ° FP16ï¼‰
        lm_head.weight.data = tensor.to(torch.float32)
    else:
        # new_key å½¢å¼å¦‚ "layers.12.self_attn.q_proj.weight"
        if new_key.startswith("layers."):
            parts = new_key.split(".")
            layer_idx = int(parts[1])               # 12..31
            new_idx = layer_idx - client_layers      # 12â†’0, 13â†’1, â€¦, 31â†’19
            mapped_key = new_key.replace(f"layers.{layer_idx}", f"layers.{new_idx}")
            server_state_dict[mapped_key] = tensor
        else:
            # norm.weight / norm.bias
            server_state_dict[new_key] = tensor
# æŠŠå 20 å±‚ + RMSNorm çš„æƒé‡åŠ è½½åˆ° stage2_net
stage2_net.load_state_dict(server_state_dict, strict=False)

# 4.5 å°†æ‰€æœ‰å­æ¨¡å—è½¬æˆ FP16 å¹¶ç§»åˆ° GPU
device = "cuda:0"
stage1_net = stage1_net.half().to(device)
stage2_net = stage2_net.half().to(device)
lm_head    = lm_head.half().to(device)
print("   â†’ Stage1/Stage2/LMHead å·²è½¬ä¸º FP16 å¹¶ç§»è‡³ GPUã€‚")

# 4.6 åˆ‡æ¢ä¸ºè¯„ä¼°æ¨¡å¼
stage1_net.eval()
stage2_net.eval()

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 5. å®šä¹‰ Split æ¨ç†å‡½æ•°ï¼šgenerate_split()
#
#    æ€è·¯ï¼š
#      1) æŠŠ prompt â†’ input_ids [1, L], attention_mask [1, L]
#      2) åœ¨æ¯ä¸€æ­¥å¾ªç¯é‡Œï¼š
#         a. Stage1: ä¼  (input_ids, attention_mask)ï¼Œä¸ä¼  position_idsï¼Œè®©å®˜æ–¹å­æ¨¡å—è‡ªåŠ¨ç”Ÿæˆ 1D pos_idsï¼Œ
#            åŒæ—¶æ‹¿åˆ° bool å‹çš„ causal_mask1 ä»¥åŠä¸­é—´ hidden1ã€‚
#         b. Stage2: ä¼  (hidden1, causal_mask1)ï¼Œä¸ä¼  position_idsï¼Œè®©å®˜æ–¹å­æ¨¡å—è‡ªåŠ¨ç»´æŒåºåˆ—é•¿åº¦ã€‚
#         c. hidden2 â†’ lm_head â†’ logits[:, -1] â†’ argmax å¾—åˆ°ä¸‹ä¸€ä¸ª token_idã€‚
#         d. æŠŠä¸‹ä¸€ä¸ª token_id æ‹¼åˆ° input_ids/attention_maskï¼Œç»§ç»­å¾ªç¯ã€‚
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def generate_split(stage1, stage2, head, tokenizer, prompt: str, max_new_tokens: int = 100):
    """
    Args:
      stage1:    LlamaStage1 å­æ¨¡å—
      stage2:    LlamaStage2 å­æ¨¡å—
      head:      å¯¹åº” lm_headï¼ˆnn.Linearï¼‰
      tokenizer: HF AutoTokenizer
      prompt:    åˆå§‹å­—ç¬¦ä¸²
      max_new_tokens: æœ€å¤šç”Ÿæˆçš„ token æ•°
    Returns:
      å®Œæ•´çš„ç”Ÿæˆï¼šprompt + æ¨¡å‹ç”Ÿæˆå†…å®¹
    """
    device = next(stage1.parameters()).device

    # 1) æŠŠ prompt ç¼–ç æˆ input_ids [1, L] & attention_mask [1, L]
    inputs = tokenizer(prompt, return_tensors="pt")
    input_ids      = inputs["input_ids"].to(device)       # [1, L]
    attention_mask = inputs["attention_mask"].to(device)  # [1, L]

    generated = prompt
    for step in range(max_new_tokens):
        print(f"step: {step} running...")
        B, L = input_ids.shape

        with torch.no_grad():
            # 2.a) é€ç»™ Stage1ï¼Œä¸ä¼  position_ids
            out1 = stage1(
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_dict=True,
                use_cache=False
            )
            hidden1      = out1["last_hidden_state"]  # [1, L, H]
            causal_mask1 = out1["causal_mask"]        # [1, 1, L, L], dtype=torch.bool
            seq_len1     = out1["seq_len"]            # ç­‰äº L

            # 2.b) é€ç»™ Stage2ï¼Œä¸ä¼  position_ids
            hidden2 = stage2(
                hidden_states=hidden1,
                causal_mask=causal_mask1,
                seq_len=seq_len1,
                use_cache=False,
                output_attentions=False
            )  # â†’ [1, L, H]

            # 2.c) hidden2 â†’ lm_head â†’ è·å– logits æœ€åä¸€ä½ â†’ argmax å¾—åˆ° next_token_id
            logits = head(hidden2)                       # [1, L, vocab_size]
            next_token_logits = logits[:, -1, :]          # [1, vocab_size]
            next_token_id     = torch.argmax(next_token_logits, dim=-1)  # [1]
            next_id = next_token_id.item()

        # 2.d) decode ä¸ºæ–‡æœ¬ï¼Œæ‹¼æ¥åˆ° generated
        new_tok_str = tokenizer.decode(next_id)
        generated   += new_tok_str

        # 2.e) æ›´æ–° input_ids å’Œ attention_mask
        next_id_tensor = next_token_id.unsqueeze(0)                   # [1,1]
        input_ids      = torch.cat([input_ids, next_id_tensor], dim=1)  # [1, L+1]
        new_attn       = torch.ones((B, 1), dtype=attention_mask.dtype, device=device)
        attention_mask = torch.cat([attention_mask, new_attn], dim=1)     # [1, L+1]

        # å¦‚æœé‡åˆ° EOSï¼Œå°±é€€å‡º
        if next_id == tokenizer.eos_token_id:
            break

    return generated

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 6. æµ‹è¯•æ¨ç†
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
if __name__ == "__main__":
    prompt_text = "Who is Crayon Shinchan?\n"
    print("ğŸš€ å¼€å§‹ Split æ¨ç†â€¦â€¦")
    output_text = generate_split(stage1_net, stage2_net, lm_head, tokenizer, prompt_text, max_new_tokens=150)
    print("æœ€ç»ˆç”Ÿæˆï¼š")
    print(output_text)
