{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fdf5f17-3238-4847-a281-3546f9f3a0b8",
   "metadata": {},
   "source": [
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "522be91f-d248-49d8-8f0c-994c6c1a790b",
   "metadata": {},
   "source": [
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    LlamaModel,\n",
    "    LlamaConfig, Qwen2VLForConditionalGeneration\n",
    ")\n",
    "import transformers\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcded1e9-22ea-4f7f-88aa-f7b91cf39eed",
   "metadata": {},
   "source": [
    "Qwen2VLForConditionalGeneration.from_pretrained()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db19b33e-f29a-43c8-ac5f-23bb070ef6e9",
   "metadata": {},
   "source": [
    "model_dir = \"/data/Models/llama3_8B_Base\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bd4f719-34c0-4133-8288-091ead1a1b33",
   "metadata": {},
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, truncation=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "# tokenizer(\"Microservice Resource Provision is Improtant to Quality of Service and Energy Conservation\")\n",
    "tokenizer(\"Please clarify the importantce of diverse resource provision policies on microservice quality of service and run-time energy conservation.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c36d89-c60d-4d60-9fb6-8f719a28581a",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb0ff5d0-b5f0-4bff-92fe-f02884cbf4cb",
   "metadata": {},
   "source": [
    "LLM_model = LlamaForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ")\n",
    "base_model = LLM_model"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f2e705-95c7-4f81-afc7-c2e77335f2ae",
   "metadata": {},
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=LLM_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256,\n",
    "    temperature=0.05,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1\n",
    ")\n",
    "\n",
    "task_agent = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "orchestrator_agent_prompt_template = \"\"\"\n",
    "You are an expert in wireless communication developed by Nokia Bell Labs China. Below is a query that describes a task of communication. Please give your response.\n",
    "### Queryï¼š\n",
    "{query}\n",
    "### Response:\n",
    "\"\"\"\n",
    "orchestrator_prompt_template = PromptTemplate(template=orchestrator_agent_prompt_template, input_variables=[\"query\"])\n",
    "OrchestratorAgent = LLMChain(prompt=orchestrator_prompt_template, llm=task_agent)\n",
    "orchestrator_out = OrchestratorAgent.run(query=q)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bef32144-4a77-4ff7-8a88-03bb94104d3e",
   "metadata": {},
   "source": [
    "LLM_model.lm_head\n",
    "# LLM_model.get_input_embeddings"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81ee0ea4-6a97-4fee-aca8-e212a1861ca8",
   "metadata": {},
   "source": [
    "LLM_model.model.layers[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b313f2-81ba-4bcc-a431-a6afd8775e4b",
   "metadata": {},
   "source": [
    "LLM_model.model.layers[1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72d510e3-ecde-4954-9230-054bf9ab174a",
   "metadata": {},
   "source": [
    "LLM_model.model.layers[-1].self_attn"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcf3a4d6-0b71-4f58-a171-a5dff578891e",
   "metadata": {},
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_dir,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    max_length=128,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "316367ff-d92e-4a1d-ba5e-61160ea2bcee",
   "metadata": {},
   "source": [
    "pipeline(\"Show me a story about thief\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62ea5ac-d341-4f05-bca7-6ee0ed73e316",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
