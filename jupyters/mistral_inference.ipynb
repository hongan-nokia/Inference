{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab26adde-244d-4696-99cf-ac36e1f23971",
   "metadata": {},
   "source": [
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "from mistral_common.protocol.instruct.messages import UserMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b4880e9-8c4f-400a-8b0e-97c6b97d62c7",
   "metadata": {},
   "source": [
    "mistral_models_path = \"/data/Models/mixtral-8x7B-Instruct-v0.1\"\n",
    " \n",
    "tokenizer = MistralTokenizer.v1()\n",
    " \n",
    "completion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n",
    "\n",
    "tokens = tokenizer.encode_chat_completion(completion_request).tokens"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad0a613e-6510-4900-b149-e086ef4cd7f8",
   "metadata": {},
   "source": [
    "from mistral_inference.transformer import Transformer\n",
    "from mistral_inference.generate import generate\n",
    "\n",
    "model = Transformer.from_folder(mistral_models_path)\n",
    "out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
    "\n",
    "result = tokenizer.decode(out_tokens[0])\n",
    "\n",
    "print(result)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e59f8-d92b-43f1-aa19-9dd741884cd8",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "add22be4-a094-4418-b317-50067ef419f7",
   "metadata": {},
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"/data/Models/mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4018e76-1e69-47a3-a09e-5ffa999d9fe1",
   "metadata": {},
   "source": [
    "model_dir = \"/data/Models/mixtral-8x7B-Instruct-v0.1\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de05777a-fa65-4e6b-88e0-f19c5a6ebd94",
   "metadata": {},
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11457aee-c984-44d5-995b-bd2706c8a058",
   "metadata": {},
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True, device_map='auto')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aeec291-ce05-444d-834c-6b8bd537ad98",
   "metadata": {},
   "source": [
    "question = \"Explain Machine Learning to me in a nutshell.\"\n",
    "\n",
    "tokens = tokenizer(question, return_tensors='pt').to(model.device)\n",
    "tokens"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99e9d79f-f9ed-44ba-9ac3-1945698c0400",
   "metadata": {},
   "source": [
    "generated_ids = model.generate(tokens, max_new_tokens=128, do_sample=True)\n",
    "\n",
    "# decode with mistral tokenizer\n",
    "result = tokenizer.decode(generated_ids[0].tolist())\n",
    "print(result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a0a48-1fe4-4887-b219-b270e10221d4",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "mistral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
