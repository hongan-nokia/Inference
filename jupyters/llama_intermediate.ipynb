{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fdf5f17-3238-4847-a281-3546f9f3a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522be91f-d248-49d8-8f0c-994c6c1a790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    pipeline,\n",
    "    AutoTokenizer,AutoModelForCausalLM,\n",
    "    LlamaModel,\n",
    "    LlamaConfig, Qwen2VLForConditionalGeneration\n",
    ")\n",
    "import transformers\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db19b33e-f29a-43c8-ac5f-23bb070ef6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/data/Models/llama3_8B_Base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd4f719-34c0-4133-8288-091ead1a1b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, truncation=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb0ff5d0-b5f0-4bff-92fe-f02884cbf4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = LlamaForCausalLM.from_pretrained(\n",
    "#     model_dir,\n",
    "#     load_in_8bit=False,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map='auto',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04c36d89-c60d-4d60-9fb6-8f719a28581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_outputs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "392fef1c-9536-4bae-b454-a936e11e5828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_func(module, inputs, outputs, layer_idx):\n",
    "    intermediate_outputs[layer_idx] = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf0c6faa-15eb-429d-bec5-1417e186d621",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx, layer in enumerate(base_model.model.layers):\n",
    "    layer.register_forward_hook(\n",
    "        lambda m, inp, out, idx=layer_idx: hook_func(m, inp, out, idx)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed4188f6-4f0e-49cb-b5d3-2eb771d26992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "question = \"Introduce UT Austin to me\"\n",
    "\n",
    "inputs = tokenizer(question, return_tensors='pt').to(base_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    response = base_model.generate(inputs.input_ids, max_length=128)\n",
    "# print(response)\n",
    "# print(tokenizer.decode(response.cpu()[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7944dce3-6b61-41b2-991a-13b1577dd94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: torch.Size([1, 1, 4096])\n",
      "Layer 1: torch.Size([1, 1, 4096])\n",
      "Layer 2: torch.Size([1, 1, 4096])\n",
      "Layer 3: torch.Size([1, 1, 4096])\n",
      "Layer 4: torch.Size([1, 1, 4096])\n",
      "Layer 5: torch.Size([1, 1, 4096])\n",
      "Layer 6: torch.Size([1, 1, 4096])\n",
      "Layer 7: torch.Size([1, 1, 4096])\n",
      "Layer 8: torch.Size([1, 1, 4096])\n",
      "Layer 9: torch.Size([1, 1, 4096])\n",
      "Layer 10: torch.Size([1, 1, 4096])\n",
      "Layer 11: torch.Size([1, 1, 4096])\n",
      "Layer 12: torch.Size([1, 1, 4096])\n",
      "Layer 13: torch.Size([1, 1, 4096])\n",
      "Layer 14: torch.Size([1, 1, 4096])\n",
      "Layer 15: torch.Size([1, 1, 4096])\n",
      "Layer 16: torch.Size([1, 1, 4096])\n",
      "Layer 17: torch.Size([1, 1, 4096])\n",
      "Layer 18: torch.Size([1, 1, 4096])\n",
      "Layer 19: torch.Size([1, 1, 4096])\n",
      "Layer 20: torch.Size([1, 1, 4096])\n",
      "Layer 21: torch.Size([1, 1, 4096])\n",
      "Layer 22: torch.Size([1, 1, 4096])\n",
      "Layer 23: torch.Size([1, 1, 4096])\n",
      "Layer 24: torch.Size([1, 1, 4096])\n",
      "Layer 25: torch.Size([1, 1, 4096])\n",
      "Layer 26: torch.Size([1, 1, 4096])\n",
      "Layer 27: torch.Size([1, 1, 4096])\n",
      "Layer 28: torch.Size([1, 1, 4096])\n",
      "Layer 29: torch.Size([1, 1, 4096])\n",
      "Layer 30: torch.Size([1, 1, 4096])\n",
      "Layer 31: torch.Size([1, 1, 4096])\n"
     ]
    }
   ],
   "source": [
    "for layer_idx, tensor in intermediate_outputs.items():\n",
    "    print(f\"Layer {layer_idx}: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16e6dd1c-b755-4f95-850b-08ff5c6f6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM_model = LlamaForCausalLM.from_pretrained(\n",
    "#     model_dir,\n",
    "#     load_in_8bit=False,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map='auto',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05ade840-c222-429a-9fca-a23be6f14c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "LLM_model = LlamaForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
