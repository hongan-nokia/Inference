class color:
    PURPLE = '\033[95m'
    CYAN = '\033[96m'
    DARKCYAN = '\033[36m'
    BLUE = '\033[94m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    END = '\033[0m'


from transformers import (
    LlamaForCausalLM,
    pipeline,
    AutoTokenizer,
    LlamaModel,
    LlamaConfig, Qwen2VLForConditionalGeneration
)
import transformers
import json

import torch
from langchain import PromptTemplate, LLMChain
from langchain.llms import HuggingFacePipeline


Qwen2VLForConditionalGeneration.from_pretrained()


model_dir = "/data/Models/llama3_8B_Base"


tokenizer = AutoTokenizer.from_pretrained(model_dir, truncation=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"
# tokenizer("Microservice Resource Provision is Improtant to Quality of Service and Energy Conservation")
tokenizer("Please clarify the importantce of diverse resource provision policies on microservice quality of service and run-time energy conservation.")





LLM_model = LlamaForCausalLM.from_pretrained(
    model_dir,
    load_in_8bit=False,
    torch_dtype=torch.float16,
    device_map='auto',
)
base_model = LLM_model


pipe = pipeline(
    "text-generation",
    model=LLM_model,
    tokenizer=tokenizer,
    max_length=256,
    temperature=0.05,
    top_p=0.9,
    repetition_penalty=1
)

task_agent = HuggingFacePipeline(pipeline=pipe)

orchestrator_agent_prompt_template = """
You are an expert in wireless communication developed by Nokia Bell Labs China. Below is a query that describes a task of communication. Please give your response.
### Queryï¼š
{query}
### Response:
"""
orchestrator_prompt_template = PromptTemplate(template=orchestrator_agent_prompt_template, input_variables=["query"])
OrchestratorAgent = LLMChain(prompt=orchestrator_prompt_template, llm=task_agent)
orchestrator_out = OrchestratorAgent.run(query=q)


LLM_model.lm_head
# LLM_model.get_input_embeddings


LLM_model.model.layers[0]


LLM_model.model.layers[1]


LLM_model.model.layers[-1].self_attn


pipeline = transformers.pipeline(
    "text-generation",
    model=model_dir,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device_map="auto",
    max_length=128,
)


pipeline("Show me a story about thief")



