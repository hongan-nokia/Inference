from transformers import AutoModelForCausalLM, AutoTokenizer


model_dir = "/data/Models/Llama-3.2-3B-Instruct-QLORA_INT4_EO8"


model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True, device_map='auto')
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)


question = "Introduce UT Austin to me"


inputs = tokenizer(question, return_tensors='pt').to(model.device)
response = model.generate(inputs.input_ids, max_length=128)
response


print(tokenizer.decode(response.cpu()[0], skip_special_tokens=True))



