model:
  base_model: "/data/Models/llama2_7B_chat_hf"
  load_8bit: false
data:
  data_path: "./commonsense_170k.json"
  cutoff_len: 256
  val_set_size: 120
training:
  output_dir: "./all_layers"
  batch_size: 16
  micro_batch_size: 16
  num_epochs: 3
  learning_rate: 0.0002
  weight_decay: 0.0
  use_gradient_checkpointing: false
  eval_step: 80
  save_step: 80
lora:
  adapter_name: "lora"
  lora_r: 32
  lora_alpha: 64
  lora_layers: 32
  lora_dropout: 0.05
bottleneck:
  bottleneck_size: 256
  non_linearity: "tanh"
  adapter_dropout: 0.0
  use_parallel_adapter: False
  use_adapterp: False
  target_modules: ["q_proj", "k_proj", "v_proj", "up_proj", "down_proj"]
llm:
  train_on_inputs: True
  group_by_length: False
wandb:
  wandb_project: "llama2_7b_chat"
  wandb_run_name: ""
  wandb_watch: ""
  wandb_log_model: ""
checkpoint:
  resume_from_checkpoint: ""