{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fdf5f17-3238-4847-a281-3546f9f3a0b8",
   "metadata": {},
   "source": [
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522be91f-d248-49d8-8f0c-994c6c1a790b",
   "metadata": {},
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    pipeline,\n",
    "    AutoTokenizer,AutoModelForCausalLM,\n",
    "    LlamaModel,\n",
    "    LlamaConfig, Qwen2VLForConditionalGeneration\n",
    ")\n",
    "import transformers\n",
    "import json\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db19b33e-f29a-43c8-ac5f-23bb070ef6e9",
   "metadata": {},
   "source": [
    "model_dir = \"/data/Models/llama3_8B_Base\"\n",
    "dataset_dir = \"/data/Datasets/nyu-mll_glue\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6871f40b-1f5c-4872-82ac-25d2202123a4",
   "metadata": {},
   "source": [
    "# 加载 GLUE 数据集，默认为 'sst2' 任务，可以换成其他任务如 'mnli'、'qqp' 等\n",
    "dataset = load_dataset(dataset_dir, \"cola\")\n",
    "# 获取测试集\n",
    "test_set = dataset[\"test\"]\n",
    "# 打印测试集的前10条数据\n",
    "# test_set\n",
    "# for i in range(10):\n",
    "#     print(test_set[i])\n",
    "test_sentences = [test_set[i]['sentence'] for i in range(30)]\n",
    "test_sentences"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd4f719-34c0-4133-8288-091ead1a1b33",
   "metadata": {},
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, truncation=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map='auto',\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0ff5d0-b5f0-4bff-92fe-f02884cbf4cb",
   "metadata": {},
   "source": [
    "# base_model = LlamaForCausalLM.from_pretrained(\n",
    "#     model_dir,\n",
    "#     load_in_8bit=False,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map='auto',\n",
    "# )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04c36d89-c60d-4d60-9fb6-8f719a28581a",
   "metadata": {},
   "source": [
    "intermediate_outputs = {}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "392fef1c-9536-4bae-b454-a936e11e5828",
   "metadata": {},
   "source": [
    "def hook_func(module, inputs, outputs, layer_idx):\n",
    "    intermediate_outputs[layer_idx] = outputs[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf0c6faa-15eb-429d-bec5-1417e186d621",
   "metadata": {},
   "source": [
    "for layer_idx, layer in enumerate(base_model.model.layers):\n",
    "    layer.register_forward_hook(\n",
    "        lambda m, inp, out, idx=layer_idx: hook_func(m, inp, out, idx)\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed4188f6-4f0e-49cb-b5d3-2eb771d26992",
   "metadata": {},
   "source": [
    "question = \"Introduce UT Austin to me\"\n",
    "\n",
    "inputs = tokenizer(test_sentences, padding=True, truncation=True, return_tensors='pt').to(base_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    response = base_model.generate(inputs.input_ids, max_length=128)\n",
    "# print(response)\n",
    "# print(tokenizer.decode(response.cpu()[0], skip_special_tokens=True))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7944dce3-6b61-41b2-991a-13b1577dd94b",
   "metadata": {},
   "source": [
    "# for layer_idx, tensor in intermediate_outputs.items():\n",
    "#     print(f\"Layer {layer_idx}: {tensor.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baf45f95-1cd3-4264-bb92-88b8ac8d3f44",
   "metadata": {},
   "source": [
    "intermediate_outputs[0][:,0,:].shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4438b585-ae2f-4ece-8562-e44efef2b755",
   "metadata": {},
   "source": [
    "## Zero-dimension Persistent Homology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22793a2-ddae-4cf7-8ac0-0dd753aa6779",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import scipy.spatial.distance as dist\n",
    "import gudhi as gd\n",
    "import matplotlib.pyplot as plt\n",
    "from persim import plot_diagrams\n",
    "from ripser import ripser\n",
    "from ripser import Rips\n",
    "import persim"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06ac1a37-3dba-47cd-8a74-e31e4911c7e2",
   "metadata": {},
   "source": [
    "def compute_rf_vr(pm_data):\n",
    "    rips = Rips(maxdim=2)\n",
    "    dgm = rips.fit_transform(pm_data)\n",
    "    H0 = dgm[0][:-1]\n",
    "    if len(H0) > 1:\n",
    "        r_f = np.max(H0[:, 1])  # 取最大的死亡时间（即最小连通分量合并半径）\n",
    "    else:\n",
    "        r_f = 0  # 如果只有一个分量，说明已经连通\n",
    "    return r_f, dgm"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db5ab16f-0881-4715-812f-80c377ad5146",
   "metadata": {},
   "source": [
    "pm_data = np.array(intermediate_outputs[26][:,0,:].cpu())\n",
    "\n",
    "# Instantiate Vietoris-Rips solver\n",
    "rips = Rips(maxdim=2)\n",
    "dgm = rips.fit_transform( pm_data)\n",
    "\n",
    "plt.figure(figsize=(5, 5), dpi=80)\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "persim.plot_diagrams(dgm, title=\"Persistence Diagram\")\n",
    "# print(f\"rips.r_cover_ is ... {rips.r_cover_}\")\n",
    "# plt.savefig(\"homology_example_persistence-diagram.png\", dpi='figure', format=None, metadata=None,\n",
    "#         bbox_inches=None, pad_inches=0.1,\n",
    "#         facecolor='white', edgecolor='auto')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "999cba5e-b8c9-4231-bf45-c8bf14fedd1f",
   "metadata": {},
   "source": [
    "r_f_list = []\n",
    "for layer_idx, tensor in intermediate_outputs.items():\n",
    "    # 计算最小半径\n",
    "    r_f, _ = compute_rf_vr(tensor[:, 0, :].cpu())\n",
    "    r_f_list.append(r_f)\n",
    "    # print(f\"Layer {layer_idx} -- radis -- {r_f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e82c3751-62d8-42d4-97f0-e4e49231e5d1",
   "metadata": {},
   "source": [
    "#---------------------------------------------------------------------------------------------------\n",
    "p_num = len(base_model.model.layers)\n",
    "x = np.linspace(0, p_num, 1)  # 在 0 到 10 之间生成 100 个等间距的点\n",
    "x = np.arange(p_num)\n",
    "y = r_f_list  \n",
    "\n",
    "plt.plot(x, y, label=\"Llama3-8B-Base\", color='b', linestyle='-')\n",
    "plt.scatter(x, y, color='r', marker='o')\n",
    "plt.title(\"Zero-dimension Persistent Homology\")\n",
    "plt.xlabel(\"Layers\")\n",
    "plt.ylabel(\"Radius\")\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ade840-c222-429a-9fca-a23be6f14c37",
   "metadata": {},
   "source": [
    "# LLM_model = LlamaForCausalLM.from_pretrained(\n",
    "#     model_dir,\n",
    "#     load_in_8bit=False,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map='auto',\n",
    "# )"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
