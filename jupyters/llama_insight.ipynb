{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fdf5f17-3238-4847-a281-3546f9f3a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "522be91f-d248-49d8-8f0c-994c6c1a790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    LlamaModel,\n",
    "    LlamaConfig, Qwen2VLForConditionalGeneration\n",
    ")\n",
    "import transformers\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcded1e9-22ea-4f7f-88aa-f7b91cf39eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qwen2VLForConditionalGeneration.from_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db19b33e-f29a-43c8-ac5f-23bb070ef6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/data/Models/llama3_8B_Base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bd4f719-34c0-4133-8288-091ead1a1b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 5618, 38263, 279, 3062, 346, 315, 17226, 5211, 17575, 10396, 389, 8162, 8095, 4367, 315, 2532, 323, 1629, 7394, 4907, 29711, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, truncation=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "# tokenizer(\"Microservice Resource Provision is Improtant to Quality of Service and Energy Conservation\")\n",
    "tokenizer(\"Please clarify the importantce of diverse resource provision policies on microservice quality of service and run-time energy conservation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c36d89-c60d-4d60-9fb6-8f719a28581a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb0ff5d0-b5f0-4bff-92fe-f02884cbf4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████| 4/4 [00:02<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "LLM_model = LlamaForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ")\n",
    "base_model = LLM_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f2e705-95c7-4f81-afc7-c2e77335f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=LLM_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256,\n",
    "    temperature=0.05,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1\n",
    ")\n",
    "\n",
    "task_agent = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "orchestrator_agent_prompt_template = \"\"\"\n",
    "You are an expert in wireless communication developed by Nokia Bell Labs China. Below is a query that describes a task of communication. Please give your response.\n",
    "### Query：\n",
    "{query}\n",
    "### Response:\n",
    "\"\"\"\n",
    "orchestrator_prompt_template = PromptTemplate(template=orchestrator_agent_prompt_template, input_variables=[\"query\"])\n",
    "OrchestratorAgent = LLMChain(prompt=orchestrator_prompt_template, llm=task_agent)\n",
    "orchestrator_out = OrchestratorAgent.run(query=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bef32144-4a77-4ff7-8a88-03bb94104d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4096, out_features=128256, bias=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM_model.lm_head\n",
    "# LLM_model.get_input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81ee0ea4-6a97-4fee-aca8-e212a1861ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaDecoderLayer(\n",
       "  (self_attn): LlamaAttention(\n",
       "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "  )\n",
       "  (mlp): LlamaMLP(\n",
       "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM_model.model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b313f2-81ba-4bcc-a431-a6afd8775e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaDecoderLayer(\n",
       "  (self_attn): LlamaAttention(\n",
       "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "  )\n",
       "  (mlp): LlamaMLP(\n",
       "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM_model.model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72d510e3-ecde-4954-9230-054bf9ab174a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaAttention(\n",
       "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM_model.model.layers[-1].self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcf3a4d6-0b71-4f58-a171-a5dff578891e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████| 4/4 [00:03<00:00,  1.29it/s]\n",
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hey how are you doing today? I’m doing fine. I’m here with a little video to show you how to make the easiest and fastest slime you will ever make. It’s a fluffy slime and it’s super easy to make. You need one cup of glue, one cup of water, one cup of shaving cream, one teaspoon of baking soda and two tablespoons of contact solution. So I’m going to start by adding my glue and water. Then I’m going to add my shaving cream. Then I’m going to add my baking soda. Then I’m going to add my contact solution. So I’m going to start'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_dir,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    max_length=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "316367ff-d92e-4a1d-ba5e-61160ea2bcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Show me a story about thief who stole a purse and I will show you a story about a thief who stole a car.\\nShow me a story about a thief who stole a car and I will show you a story about a thief who stole a boat.\\nShow me a story about a thief who stole a boat and I will show you a story about a thief who stole a plane.\\nShow me a story about a thief who stole a plane and I will show you a story about a thief who stole a train.\\nShow me a story about a thief who stole a train and I will show you a story about a thief who stole a'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"Show me a story about thief\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62ea5ac-d341-4f05-bca7-6ee0ed73e316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
